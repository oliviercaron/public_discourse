{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import dateparser\n",
    "import unidecode\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_text_or_nan(driver, selector):\n",
    "    \"\"\" Try to find and return the text of an element, or NaN if not found. \"\"\"\n",
    "    try:\n",
    "        return driver.find_element(By.CSS_SELECTOR, selector).text\n",
    "    except NoSuchElementException:\n",
    "        return np.nan\n",
    "\n",
    "def scrape_page(page_num, data_folder):\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    try:\n",
    "        url = f\"https://www.vie-publique.fr/discours?page={page_num}\"\n",
    "        driver.get(url)\n",
    "        #time.sleep(random.uniform(1, 2))  # Random delay\n",
    "\n",
    "        # Saving HTML content of the list page\n",
    "        html_folder = os.path.join(data_folder, \"html_page\")\n",
    "        os.makedirs(html_folder, exist_ok=True)\n",
    "        html_filename = os.path.join(html_folder, f\"html_page_{page_num}.html\")\n",
    "        with open(html_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(driver.page_source)\n",
    "\n",
    "        # Process the page content\n",
    "        process_page_content(driver, page_num, data_folder)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {page_num}: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def process_page_content(driver, page_num, data_folder):\n",
    "    # Find all elements with the class 'fr-card__content'\n",
    "    cards = driver.find_elements(By.CLASS_NAME, \"fr-card__content\")\n",
    "    data = []\n",
    "\n",
    "    # Process each card\n",
    "    for card in cards:\n",
    "        nature = find_element_text_or_nan(card, \".fr-card__start .field__item\")\n",
    "        title = find_element_text_or_nan(card, \".fr-card__title a\")\n",
    "        link = card.find_element(By.CSS_SELECTOR, \".fr-card__title a\").get_attribute(\"href\")\n",
    "\n",
    "        try:\n",
    "            date_text = card.find_element(By.CSS_SELECTOR, \".fr-card__end time\").text\n",
    "            date = dateparser.parse(date_text).strftime(\"%d/%m/%Y\") if dateparser.parse(date_text) else np.nan\n",
    "        except NoSuchElementException:\n",
    "            date = np.nan\n",
    "\n",
    "        data.append({\"nature\": nature, \"title\": title, \"url\": link, \"date\": date})\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Loop through each link to extract additional information\n",
    "    for index, row in df.iterrows():\n",
    "        driver.get(row[\"url\"])\n",
    "\n",
    "        # Sleep for a random duration between 1 and 5 seconds (let's hope I don't get banned=>toolongdontcare)\n",
    "        #time.sleep(random.uniform(1, 2))\n",
    "\n",
    "        # Save HTML content of the current link page\n",
    "        link_html_folder = os.path.join(data_folder, \"url_html_page\")\n",
    "        if not os.path.exists(link_html_folder):\n",
    "            os.makedirs(link_html_folder)\n",
    "        link_html_filename = os.path.join(link_html_folder, f\"html_page_{page_num}_url_{index}.html\")\n",
    "        with open(link_html_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(driver.page_source)\n",
    "\n",
    "        # Extract 'tag'\n",
    "        try:\n",
    "            tag = driver.find_element(By.CLASS_NAME, \"vp-item-tag\").text\n",
    "        except NoSuchElementException:\n",
    "            tag = \"NA\"\n",
    "        df.at[index, 'tag'] = tag\n",
    "\n",
    "        # Extract 'speaker'\n",
    "        try:\n",
    "            speakers_elements = driver.find_elements(By.CSS_SELECTOR, \".vp-intervenant .line-intervenant li\")\n",
    "            speakers = []\n",
    "            for speaker_element in speakers_elements:\n",
    "                speaker_name = speaker_element.find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                speaker_title = speaker_element.text.replace(speaker_name, '').strip(\" -;\")  # Remove the name and extra characters\n",
    "                speaker_info = f\"{speaker_name} - {speaker_title}\" if speaker_title else speaker_name\n",
    "                if speaker_info not in speakers:  # Check for duplicates\n",
    "                    speakers.append(speaker_info)\n",
    "            df.at[index, 'speaker'] = \" ; \".join(speakers)\n",
    "        except NoSuchElementException:\n",
    "            df.at[index, 'speaker'] = \"NA\"\n",
    "\n",
    "        # Check if 'vp-intervenant' exists and get its position\n",
    "        try:\n",
    "            intervenant_element = driver.find_element(By.CLASS_NAME, \"vp-intervenant\")\n",
    "            intervenant_position = intervenant_element.location['y']\n",
    "        except NoSuchElementException:\n",
    "            intervenant_position = None\n",
    "\n",
    "        # Find the last <p> tag after 'vp-intervenant'\n",
    "        try:\n",
    "            all_p_elements = driver.find_elements(By.CSS_SELECTOR, \".vp-discours-details p\")\n",
    "            for p_element in reversed(all_p_elements):\n",
    "                if intervenant_position and p_element.location['y'] > intervenant_position:\n",
    "                    last_p_text = p_element.text\n",
    "                    break\n",
    "            else:\n",
    "                raise NoSuchElementException  # No <p> found after 'vp-intervenant'\n",
    "\n",
    "            column_name = unidecode.unidecode(last_p_text.split(\":\")[0].strip().split()[0].lower()) # Remove accents + lower case\n",
    "            column_value = last_p_text.split(\":\")[1].strip() if \":\" in last_p_text else \"NA\"\n",
    "            df.at[index, column_name] = column_value\n",
    "        except (NoSuchElementException, IndexError):\n",
    "            pass  # Do not create column if the element is absent or there is an error\n",
    "\n",
    "        # Extract 'text'\n",
    "        try:\n",
    "            text = driver.find_element(By.CLASS_NAME, \"field--name-field-texte-integral\").text\n",
    "        except NoSuchElementException:\n",
    "            text = \"NA\"\n",
    "        df.at[index, 'text'] = text\n",
    "\n",
    "        # Extract 'keywords'\n",
    "        try:\n",
    "            keywords_elements = driver.find_elements(By.CLASS_NAME, \"fr-tag--green-emeraude\")\n",
    "            keywords = \" ; \".join([keyword.text for keyword in keywords_elements])\n",
    "        except NoSuchElementException:\n",
    "            keywords = \"NA\"\n",
    "        df.at[index, 'keywords'] = keywords\n",
    "\n",
    "    # Save data to CSV\n",
    "    save_data_to_csv(data, page_num, data_folder)\n",
    "\n",
    "def save_data_to_csv(data, page_num, data_folder):\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_folder = os.path.join(data_folder, \"csv_page\")\n",
    "    os.makedirs(csv_folder, exist_ok=True)\n",
    "    csv_filename = os.path.join(csv_folder, f\"csv_page_{page_num}.csv\")\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Main code\n",
    "data_folder = \"data\"\n",
    "start_page = 5000\n",
    "end_page = 5050\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(scrape_page, page_num, data_folder) for page_num in range(start_page, end_page + 1)]\n",
    "    concurrent.futures.wait(futures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
